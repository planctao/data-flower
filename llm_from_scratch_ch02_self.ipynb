{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8aDWkFzPrCtGYbf4uEuoN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/planctao/data-flower/blob/main/llm_from_scratch_ch02_self.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pssZtksU62um"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YezW_Mfs7DLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载 数据集\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "b9rZDnYN7DuA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分词\n",
        "\n",
        "# 读取文本\n",
        "with open('the-verdict.txt','r') as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "J-_NUMw-7Rt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def split_sentence(text):\n",
        "    result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    result = [item.strip() for item in result if item.strip()]\n",
        "    return result"
      ],
      "metadata": {
        "id": "oqGPIbA27SlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_list = split_sentence(raw_text) # 变成单个单词的形式"
      ],
      "metadata": {
        "id": "VI-8aDFl7Tj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_list)"
      ],
      "metadata": {
        "id": "7MBEKi6p7Toq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3304cb1-573f-451e-c7e0-8ad55b2d5c12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4690"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_set = sorted(set(split_list)) # 对其进行set并去重，构建token列表\n",
        "len(sorted_set)\n",
        "\n",
        "def make_token_dict(sorted_set): # 通过构建一个dict，构建ID->token以及token->ID的映射列表\n",
        "    ret1 = {}\n",
        "    ret2 = {}\n",
        "    for i, voc in enumerate(sorted_set):\n",
        "        ret1[i] = voc\n",
        "        ret2[voc] = i\n",
        "    return ret1, ret2\n",
        "\n",
        "ID2voc, voc2ID = make_token_dict(sorted_set=sorted_set)\n",
        "\n",
        "for i in range(0, 20):\n",
        "    print(ID2voc[i],\" : \",i)"
      ],
      "metadata": {
        "id": "1EJBDi8o7Trd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb61435-6572-4c15-c16a-91ba9293a351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!  :  0\n",
            "\"  :  1\n",
            "'  :  2\n",
            "(  :  3\n",
            ")  :  4\n",
            ",  :  5\n",
            "--  :  6\n",
            ".  :  7\n",
            ":  :  8\n",
            ";  :  9\n",
            "?  :  10\n",
            "A  :  11\n",
            "Ah  :  12\n",
            "Among  :  13\n",
            "And  :  14\n",
            "Are  :  15\n",
            "Arrt  :  16\n",
            "As  :  17\n",
            "At  :  18\n",
            "Be  :  19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer1:\n",
        "    def __init__(self, id2voc, voc2id):\n",
        "        self.ID2voc = id2voc\n",
        "        self.voc2ID = voc2ID\n",
        "    def encode(self, texts):\n",
        "        def split_sentence(text):\n",
        "            result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "            result = [item.strip() for item in result if item.strip()]\n",
        "            return result\n",
        "        texts = split_sentence(texts)\n",
        "        return [voc2ID[text] for text in texts]\n",
        "    def decode(self, ids):\n",
        "        return [ID2voc[id] for id in ids]\n"
      ],
      "metadata": {
        "id": "asFJMgI6iDvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2DatasetV1(Dataset):\n",
        "    def __init__(self, txt, max_length, stride, tokenizer):\n",
        "        ids = tokenizer.encode(txt) #\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        for i in range(0, len(ids) - max_length, stride):\n",
        "            x = torch.tensor(ids[i: i + max_length])\n",
        "            y = torch.tensor(ids[i+1: i + max_length + 1])\n",
        "            self.input_ids.append(x)\n",
        "            self.target_ids.append(y)\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPT2DatasetV1(\n",
        "        txt=raw_text, max_length=256, stride=1, tokenizer=tokenizer\n",
        "    )\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=0\n",
        "    )\n",
        "    return dataloader\n",
        "dataloader = create_dataloader_v1(\n",
        "    txt = raw_text,\n",
        "    batch_size=1,\n",
        "    max_length=4,\n",
        "    stride=1,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=1\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "YPPMMk84iDzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef906d40-4ae5-4d28-82f3-b0a9802e6557"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  550, 11001,    11,   262,  2756,   286,   366,    38,   271, 10899,\n",
            "            82,     1,  1816,   510,    13,   198,   198,  1026,   373,   407,\n",
            "         10597,  1115,   812,  1568,   326,    11,   287,   262,  1781,   286,\n",
            "           257,  1178,  2745,     6,  4686,  1359,   319,   262, 34686, 41976,\n",
            "            11,   340,  6451,  5091,   284,   502,   284,  4240,  1521,   402,\n",
            "           271, 10899,   550,  1813,   510,   465, 12036,    13,  1550, 14580,\n",
            "            11,   340,  1107,   373,   257, 29850,  1917,    13,  1675, 24456,\n",
            "           465,  3656,   561,   423,   587,  1165,  2562,   438, 14363,  3148,\n",
            "          1650,  1010,   550,   587,  6699,   262,  1540,   558,   286,  2282,\n",
            "           326,  9074,    13,   402,   271, 10899,   550,   366,  7109, 14655,\n",
            "           683,   866,   526,  1114,  9074,    13,   402,   271, 10899,   438,\n",
            "           292,   884,   438, 18108,   407, 11196, 10597,  3016,   257,   614,\n",
            "           706,  3619,   338, 10568,   550,   587,  2077,    13,   632,  1244,\n",
            "           307,   326,   339,   550,  6405,   607,   438, 20777,   339,  8288,\n",
            "           465, 10152,   438, 13893,   339,  1422,   470,   765,   284,   467,\n",
            "           319, 12036,    26,   475,   340,   561,   423,   587,  1327,   284,\n",
            "          5879,   326,   339,   550,  1813,   510,   465, 12036,   780,   339,\n",
            "           550,  6405,   607,    13,   198,   198,  5189,  1781,    11,   611,\n",
            "           673,   550,   407, 17901,   683,   866,    11,   673,   550,  8603,\n",
            "            11,   355,  4544,  9325,   701, 42397,    11,  4054,   284,   366,\n",
            "         26282,   683,   510,     1,   438,  7091,   550,   407,  2957,   683,\n",
            "           736,   284,   262,  1396,   417,    13,  1675,  1234,   262, 14093,\n",
            "           656,   465,  1021,   757,   438, 10919,   257,   410,  5040,   329,\n",
            "           257,  3656,     0,   887,  9074,    13,   402,   271, 10899,  4120,\n",
            "           284,   423,   595,    67,  1328,   340,   438,   392,   314,  2936,\n",
            "           340,  1244,   307,  3499,   284,  1064]]), tensor([[11001,    11,   262,  2756,   286,   366,    38,   271, 10899,    82,\n",
            "             1,  1816,   510,    13,   198,   198,  1026,   373,   407, 10597,\n",
            "          1115,   812,  1568,   326,    11,   287,   262,  1781,   286,   257,\n",
            "          1178,  2745,     6,  4686,  1359,   319,   262, 34686, 41976,    11,\n",
            "           340,  6451,  5091,   284,   502,   284,  4240,  1521,   402,   271,\n",
            "         10899,   550,  1813,   510,   465, 12036,    13,  1550, 14580,    11,\n",
            "           340,  1107,   373,   257, 29850,  1917,    13,  1675, 24456,   465,\n",
            "          3656,   561,   423,   587,  1165,  2562,   438, 14363,  3148,  1650,\n",
            "          1010,   550,   587,  6699,   262,  1540,   558,   286,  2282,   326,\n",
            "          9074,    13,   402,   271, 10899,   550,   366,  7109, 14655,   683,\n",
            "           866,   526,  1114,  9074,    13,   402,   271, 10899,   438,   292,\n",
            "           884,   438, 18108,   407, 11196, 10597,  3016,   257,   614,   706,\n",
            "          3619,   338, 10568,   550,   587,  2077,    13,   632,  1244,   307,\n",
            "           326,   339,   550,  6405,   607,   438, 20777,   339,  8288,   465,\n",
            "         10152,   438, 13893,   339,  1422,   470,   765,   284,   467,   319,\n",
            "         12036,    26,   475,   340,   561,   423,   587,  1327,   284,  5879,\n",
            "           326,   339,   550,  1813,   510,   465, 12036,   780,   339,   550,\n",
            "          6405,   607,    13,   198,   198,  5189,  1781,    11,   611,   673,\n",
            "           550,   407, 17901,   683,   866,    11,   673,   550,  8603,    11,\n",
            "           355,  4544,  9325,   701, 42397,    11,  4054,   284,   366, 26282,\n",
            "           683,   510,     1,   438,  7091,   550,   407,  2957,   683,   736,\n",
            "           284,   262,  1396,   417,    13,  1675,  1234,   262, 14093,   656,\n",
            "           465,  1021,   757,   438, 10919,   257,   410,  5040,   329,   257,\n",
            "          3656,     0,   887,  9074,    13,   402,   271, 10899,  4120,   284,\n",
            "           423,   595,    67,  1328,   340,   438,   392,   314,  2936,   340,\n",
            "          1244,   307,  3499,   284,  1064,   503]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zU-pKmXqiD12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixSST9HliD4X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}